{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ è§†é¢‘ä¿¡æ¯: æ—¶é•¿ 4.00s | æ€»å¸§æ•° 92 | FPS 23\n",
      "æŠ½å– 20 å¸§ç”¨äºåˆ†æ\n",
      "æ¨ç†ä¸­\n",
      "âŒ æ¨ç†å‡ºé”™: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import ollama\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "MODEL_NAME = \"qwen2.5vl:32b\"  \n",
    "\n",
    "# VIDEO_PATH = \"./test_videos/video_105.mp4\" \n",
    "VIDEO_PATH = \"./output_L2CS/video_105.mp4\" \n",
    "# VIDEO_PATH = \"./output_gazelle/video_105.mp4\" \n",
    "\n",
    "# å¦‚æœè§†é¢‘é•¿ 10 ç§’ï¼ŒSAMPLES=10 ä»£è¡¨æ¯ç§’æˆªä¸€å¼ \n",
    "TARGET_FRAME_COUNT = 20 \n",
    "# ===========================================\n",
    "\n",
    "def extract_frames(video_path, target_count=TARGET_FRAME_COUNT):\n",
    "    \"\"\"\n",
    "    ä»è§†é¢‘ä¸­å‡åŒ€æŠ½å–æŒ‡å®šæ•°é‡çš„å¸§ï¼Œå¹¶è½¬æ¢ä¸ºå­—èŠ‚æµ\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    print(f\"ğŸ¬ è§†é¢‘ä¿¡æ¯: æ—¶é•¿ {duration:.2f}s | æ€»å¸§æ•° {total_frames} | FPS {fps}\")\n",
    "    \n",
    "    # è®¡ç®—æŠ½å¸§é—´éš”\n",
    "    step = max(1, total_frames // target_count)\n",
    "    \n",
    "    frame_images = [] \n",
    "    \n",
    "    count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # æŒ‰ç…§æ­¥é•¿æŠ½å¸§\n",
    "        if count % step == 0 and saved_count < target_count:\n",
    "            # OpenCV é»˜è®¤æ˜¯ BGRï¼Œéœ€è¦è½¬ä¸º RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # ä¸ºäº†åŠ å¿«é€Ÿåº¦å’ŒèŠ‚çœæ˜¾å­˜ï¼Œå¯ä»¥å°†å›¾ç‰‡ç¼©å°ï¼Œä¾‹å¦‚æœ€å¤§è¾¹é•¿ 720 æˆ– 512\n",
    "            pil_img.thumbnail((960, 540)) \n",
    "            \n",
    "            # è½¬ä¸ºå­—èŠ‚æµ\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            pil_img.save(img_byte_arr, format='JPEG', quality=90)\n",
    "            image_bytes = img_byte_arr.getvalue()\n",
    "            \n",
    "            frame_images.append(image_bytes)\n",
    "            saved_count += 1\n",
    "            \n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"æŠ½å– {len(frame_images)} å¸§ç”¨äºåˆ†æ\")\n",
    "    return frame_images\n",
    "\n",
    "def analyze_video(video_path):\n",
    "    frames = extract_frames(video_path, target_count=TARGET_FRAME_COUNT)\n",
    "    \n",
    "    if not frames:\n",
    "        return\n",
    "\n",
    "    prompt = (     \n",
    "            \"You are an expert in micro-expression analysis and social psychology. \"\n",
    "            # \"Your goal is to detect 'Social Masking'â€”where a person hides their true negative emotions behind a fake smile.\\n\\n\"\n",
    "            \"Your task is to interpret a personâ€™s body language.\"\n",
    "            \n",
    "            \"You must analyze the video frames step-by-step using the following strict reasoning pipeline:\\n\\n\"\n",
    "            \n",
    "            \"### Step 1: Expression Recognition\\n\"\n",
    "            \"- Observe the mouth and cheeks. Fact: [The speaker's facial expression is animated and expressive.]\\n\"\n",
    "            # \"- Check the eyes. Is it a 'Duchenne Smile' (genuine joy with eye crinkles) or a 'Social Smile' (mouth only)?\\n\\n\"\n",
    "            \n",
    "            \"### Step 2: Gaze Dynamics Analysis (Crucial)\\n\"\n",
    "            \"- Focus strictly on the movements of the red arrow vectors (with directions) indicating BOTH the speaker(right) and listener(left)'s gaze movements. \\n\"\n",
    "            # \"- Focus strictly on the movements of the red arrow vectors (with directions) indicating the listener (on the left)'s gaze movements. \\n\"\n",
    "            # \"- Does the gaze shifts (looking up, down, or sideways...) while the expression persists?\\n\"\n",
    "            \"- Does the gaze shifts (looking up, down, or sideways...) \\n\"\n",
    "            \n",
    "            # \"- Does the speaker maintain steady eye contact with the listener?\\n\"\n",
    "            # \"- Note: 'Gaze Aversion' often indicates cognitive load, lying, or discomfort.\\n\\n\"\n",
    "            \n",
    "            \"### Step 3: Conflict Detection (The 'Incongruence')\\n\"\n",
    "            \"- Compare Step 1 and Step 2. Is the expression consistent with the gaze behavior? or conflict?\\n\"\n",
    "            # \"- If she is smiling but looking away/down, this is a Conflict (Incongruence).\\n\\n\"\n",
    "            \n",
    "            \"### Step 4: Final Inference\\n\"\n",
    "            \"- Based on the conflict, determine the true mental state.\\n\"\n",
    "            \"- Choose between:\\n\"\n",
    "            \"   A. [His body language and facial expressions indicate a positive and enthusiastic emotion, likely joy.] \\n\"\n",
    "            \"   B. [His body language and facial expressions do not indicate a positive and enthusiastic emotion, likely joy.] \\n\\n\"\n",
    "            \n",
    "            \"Output your reasoning for each step, and then provide the final conclusion.\"\n",
    "        )\n",
    "\n",
    "    # prompt = (     \n",
    "    #     f\"You are a socially intelligent body language expert. Your task is to interpret a personâ€™s body language. \"\n",
    "    #     f\"You will be given a set of video frames representing a clip with one main speaker. \" \n",
    "    #     f\"Follow the Task Guidelines and the Response Format.\\n\"            \n",
    "    #     f\"- You are given one fact about the speakerâ€™s body language, and two possible interpretations.\\n\"\n",
    "    #     f\"- Think out loud about which interpretation is better given what you see in the images (2-3 sentences).\\n\"\n",
    "    #     # add gaze\n",
    "    #     f\"- Please carefully observe the trajectory of the speaker's gaze (indicated by the red arrows) and eye blink in the whole video and think about it\\n\\n\"\n",
    "    #     f\"- Finally, give your answer according to the Response Format.\\n\\n\"\n",
    "    #     f\"Thinking out loud: <your thoughts about which interpretation is better (2-3 sentences)> \\n\"\n",
    "    #     f\"Answer: <A OR B>\\n\\n\"\n",
    "    #     f\"### Fact: [She is smiling slightly.] \\n\\n\"\n",
    "    #     f\"### Interpretations\\n\"\n",
    "    #     f\"A. [Based on these observations, the speaker is likely not feeling joy.] \\n\"\n",
    "    #     f\"B. [Based on these observations, the speaker is likely feeling joy.] \\n\\n\"\n",
    "    #     f\"Please analyze the possible problems and give suggestions for improvement and rehabilitation actions.\"\n",
    "    # )\n",
    "\n",
    "    # prompt = (\"describe what you have seen in the images in detail.\")\n",
    "\n",
    "    print(\"æ¨ç†ä¸­\")\n",
    "    \n",
    "    # 3. å‘é€ç»™ Ollama:'images' å‚æ•°æ¥å—ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬æŠŠæ‰€æœ‰æŠ½å‡ºæ¥çš„å¸§éƒ½æ”¾è¿›å»\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'images': frames  # è¿™é‡Œä¼ å…¥æ‰€æœ‰å¸§\n",
    "            }],\n",
    "                options={\n",
    "                    'temperature': 0.1,\n",
    "                    'num_ctx': 20000\n",
    "                }\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*20 + \" è¯†åˆ«ç»“æœ \" + \"=\"*20)\n",
    "        print(response['message']['content'])\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨ç†å‡ºé”™: {e}\")\n",
    "\n",
    "\n",
    "analyze_video(VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze-Derivative Keyframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†æè§†é¢‘ Gaze å˜åŒ–\n",
      "ğŸ—‘ï¸ å·²æ¸…ç©ºæ—§æ–‡ä»¶å¤¹: ./key_frames_output\n",
      "ğŸ“ åˆ›å»ºæ–°æ–‡ä»¶å¤¹: ./key_frames_output\n",
      "ğŸ’¾ æ­£åœ¨ä¿å­˜ 33 å¼ å…³é”®å¸§...\n",
      "âœ… å…³é”®å¸§ä¿å­˜å®Œæ¯•ã€‚è·¯å¾„: ./key_frames_output\n",
      "é€‰å‡º 33 ä¸ªå…³é”®å¸§ (åŒ…å« Gaze çªå˜ç‚¹): [0, np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(12), np.int64(13), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(48), np.int64(49), np.int64(52), np.int64(53), np.int64(56), np.int64(57), np.int64(58), 80]\n",
      "ğŸ¤– å‘é€ç»™ VLM æ¨ç†ä¸­...\n",
      "\n",
      "==============================\n",
      "### Step 0: Fact\n",
      "- Given the fact: [She is smiling slightly.]\n",
      "\n",
      "### Step 1: Vector Trajectory Tracing (The 'What')\n",
      "- **Key Question**: Define the 'Gaze Pattern'.\n",
      "  - The red arrow in the frames shows a slight downward movement, then a slight rightward movement, and finally a slight upward movement.\n",
      "  - The gaze appears to be wandering slightly, but it is not pointing at a fixed target. It seems to be moving around the scene.\n",
      "\n",
      "### Step 2: Incongruence Check (The 'Why')\n",
      "- **Key Question**: Is the expression consistent with the gaze behavior?\n",
      "  - The woman is smiling slightly, which could be interpreted as a polite or positive expression. However, the slight downward and rightward gaze movements suggest a lack of focus on the person she is speaking to, which could indicate a lack of engagement or interest.\n",
      "\n",
      "### Step 3: Psychological Inference (The 'Meaning')\n",
      "- Based on the combination of a polite smile and wandering gaze, the true mental state is likely:\n",
      "  - **A. The speaker is not feeling joy.**\n",
      "\n",
      "### Step 4: Final Inference\n",
      "- **Final Conclusion**: Based on these observations, the speaker is likely not feeling joy. The slight smile could be a polite gesture, but the wandering gaze suggests a lack of genuine engagement or interest in the interaction.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import os\n",
    "import ollama\n",
    "from PIL import Image\n",
    "from l2cs import select_device, Pipeline, render\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# ================= é…ç½® =================\n",
    "# VIDEO_PATH = '../output_L2CS/video_24.mp4'\n",
    "VIDEO_PATH = '../test_videos/video_24.mp4'\n",
    "MODEL_NAME = \"qwen2.5vl:7b\"\n",
    "CWD = pathlib.Path.cwd()\n",
    "\n",
    "# ä¿å­˜å…³é”®å¸§çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "OUTPUT_KEYFRAMES_DIR = './key_frames_output' \n",
    "\n",
    "# é˜ˆå€¼ï¼šåªæœ‰è§†çº¿è§’åº¦å˜åŒ–è¶…è¿‡è¿™ä¸ªå€¼ï¼Œæ‰è®¤ä¸ºå‘ç”Ÿäº† Shift\n",
    "SHIFT_THRESHOLD = 0.20  \n",
    "# å‘é€ç»™ VLM çš„æœ€å¤§å¸§æ•° (ä¿æŒå°‘è€Œç²¾)\n",
    "MAX_VLM_FRAMES = 20 \n",
    "# =======================================\n",
    "\n",
    "def calculate_angular_distance(pitch1, yaw1, pitch2, yaw2):\n",
    "    \"\"\"è®¡ç®—ä¸¤å¸§ä¹‹é—´è§†çº¿å‘é‡çš„æ¬§æ°è·ç¦»ï¼ˆå˜åŒ–é‡ï¼‰\"\"\"\n",
    "    return np.sqrt((pitch2 - pitch1)**2 + (yaw2 - yaw1)**2)\n",
    "\n",
    "def save_key_frames(frame_data, sorted_indices, output_dir):\n",
    "    \"\"\"\n",
    "    ä¿å­˜ç­›é€‰å‡ºçš„å…³é”®å¸§åˆ°æœ¬åœ°æ–‡ä»¶å¤¹\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        # å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨ï¼Œé€’å½’åˆ é™¤æ–‡ä»¶å¤¹åŠå…¶æ‰€æœ‰å†…å®¹\n",
    "        shutil.rmtree(output_dir)\n",
    "        print(f\"ğŸ—‘ï¸ å·²æ¸…ç©ºæ—§æ–‡ä»¶å¤¹: {output_dir}\")\n",
    "    \n",
    "    # é‡æ–°åˆ›å»ºç©ºæ–‡ä»¶å¤¹\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"ğŸ“ åˆ›å»ºæ–°æ–‡ä»¶å¤¹: {output_dir}\")\n",
    "    \n",
    "    print(f\"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(sorted_indices)} å¼ å…³é”®å¸§...\")\n",
    "    \n",
    "    saved_paths = []\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        # ä» frame_data ä¸­è·å– BGR å›¾åƒ\n",
    "        img_bgr = frame_data[idx]['image']\n",
    "        score = frame_data[idx]['score']\n",
    "        \n",
    "        # æ–‡ä»¶ååŒ…å«åºå·ã€åŸå§‹å¸§å·å’Œå˜åŒ–åˆ†æ•°ï¼Œæ–¹ä¾¿äººå·¥æ£€æŸ¥\n",
    "        filename = f\"keyframe_{i:02d}_frame_{idx:04d}_score_{score:.3f}.jpg\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        cv2.imwrite(save_path, img_bgr)\n",
    "        saved_paths.append(save_path)\n",
    "        \n",
    "    print(f\"âœ… å…³é”®å¸§ä¿å­˜å®Œæ¯•ã€‚è·¯å¾„: {output_dir}\")\n",
    "    return saved_paths\n",
    "\n",
    "def process_video_smartly(video_path):\n",
    "    if torch.cuda.is_available():\n",
    "        device_obj = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device_obj = torch.device(\"cpu\")\n",
    "        \n",
    "    # åˆå§‹åŒ– L2CS\n",
    "    gaze_pipeline = Pipeline(\n",
    "        weights=CWD / 'models' / 'L2CSNet_gaze360.pkl',\n",
    "        arch='ResNet50',\n",
    "        device=device_obj\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}\")\n",
    "        return [], []\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    frame_data = [] # å­˜å‚¨: {index, image, pitch, yaw, score}\n",
    "    \n",
    "    prev_pitch = None\n",
    "    prev_yaw = None\n",
    "    \n",
    "    print(\"åˆ†æè§†é¢‘ Gaze å˜åŒ–\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # 1. L2CS æ¨ç†\n",
    "        with torch.no_grad():\n",
    "            results = gaze_pipeline.step(frame)\n",
    "        \n",
    "        # 2. ç»˜åˆ¶ç®­å¤´ (ä¿ç•™è§†è§‰æç¤º)\n",
    "        vis_frame = render(frame, results)\n",
    "        \n",
    "        # 3. è®¡ç®—å˜åŒ–é‡\n",
    "        current_score = 0.0\n",
    "        current_pitch = 0.0\n",
    "        current_yaw = 0.0\n",
    "        \n",
    "        if len(results.pitch) > 0:\n",
    "            target_idx = 0 \n",
    "            current_pitch = results.pitch[target_idx].item()\n",
    "            current_yaw = results.yaw[target_idx].item()\n",
    "            \n",
    "            if prev_pitch is not None:\n",
    "                current_score = calculate_angular_distance(prev_pitch, prev_yaw, current_pitch, current_yaw)\n",
    "            \n",
    "            prev_pitch = current_pitch\n",
    "            prev_yaw = current_yaw\n",
    "\n",
    "        # 4. è§†è§‰å¢å¼ºï¼šå†™å…¥æ—¶é—´æˆ³\n",
    "        time_str = f\"T={frame_idx/fps:.1f}s | Frame={frame_idx}\"\n",
    "        cv2.putText(vis_frame, time_str, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        \n",
    "        # å­˜å‚¨æ•°æ®\n",
    "        frame_data.append({\n",
    "            'index': frame_idx,\n",
    "            'image': vis_frame, # å·²ç»æ˜¯ BGR\n",
    "            'score': current_score,\n",
    "            'pitch': current_pitch,\n",
    "            'yaw': current_yaw\n",
    "        })\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not frame_data:\n",
    "        print(\"âŒ æœªå¤„ç†ä»»ä½•å¸§\")\n",
    "        return [], []\n",
    "\n",
    "    # === å…³é”®æ­¥éª¤ï¼šå…³é”®å¸§ç­›é€‰ç­–ç•¥ ===\n",
    "    # ç­–ç•¥ï¼šé¦–å¸§ + å°¾å¸§ + å˜åŒ–ç‡æœ€é«˜çš„ Top-N å¸§\n",
    "    \n",
    "    # 1. æ‰¾åˆ°æ‰€æœ‰å˜åŒ–ç‡çš„å³°å€¼\n",
    "    scores = [f['score'] for f in frame_data]\n",
    "    # è·å–å˜åŒ–æœ€å¤§çš„ç´¢å¼•\n",
    "    top_indices = np.argsort(scores)[-MAX_VLM_FRAMES:] \n",
    "    \n",
    "    # 2. æ€»æ˜¯åŒ…å«å¼€å§‹å’Œç»“æŸï¼Œä¿è¯ä¸Šä¸‹æ–‡\n",
    "    final_indices = set([0, len(frame_data)-1])\n",
    "    \n",
    "    # 3. æ·»åŠ å˜åŒ–å‰§çƒˆçš„å¸§\n",
    "    for idx in top_indices:\n",
    "        if scores[idx] > SHIFT_THRESHOLD: # åªæœ‰å˜åŒ–è¶³å¤Ÿå¤§æ‰ç®—\n",
    "            final_indices.add(idx)\n",
    "            # åŒæ—¶ä¹ŸåŠ å…¥çªå˜å‰ä¸€å¸§ï¼Œæ–¹ä¾¿ VLM å¯¹æ¯” \"Before & After\"\n",
    "            if idx > 0: final_indices.add(idx - 1)\n",
    "            \n",
    "    # 4. æ’åº\n",
    "    sorted_indices = sorted(list(final_indices))\n",
    "    \n",
    "    # --- æ–°å¢ï¼šä¿å­˜å…³é”®å¸§åˆ°æœ¬åœ° ---\n",
    "    save_key_frames(frame_data, sorted_indices, OUTPUT_KEYFRAMES_DIR)\n",
    "    # ---------------------------\n",
    "\n",
    "    selected_frames_bytes = []\n",
    "    print(f\"é€‰å‡º {len(sorted_indices)} ä¸ªå…³é”®å¸§ (åŒ…å« Gaze çªå˜ç‚¹): {sorted_indices}\")\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        img_bgr = frame_data[idx]['image']\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        pil_img.thumbnail((960, 540)) # ç¼©æ”¾\n",
    "        \n",
    "        byte_arr = io.BytesIO()\n",
    "        pil_img.save(byte_arr, format='JPEG')\n",
    "        selected_frames_bytes.append(byte_arr.getvalue())\n",
    "        \n",
    "    return selected_frames_bytes, sorted_indices\n",
    "\n",
    "def vlm_reasoning(frames_bytes, indices):\n",
    "    if not frames_bytes:\n",
    "        print(\"æœªæå–åˆ°å¸§\")\n",
    "        return\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are an expert in micro-expression analysis. \\n\"\n",
    "        # \"You are analyzing a sequence of video frames marked with RED ARROWS indicating the gaze direction of the woman.\\n\\n\"\n",
    "\n",
    "        \"Your task is to interpret a personâ€™s body language.\"\n",
    "        \"You will be given a set of video frames representing a clip with one main speaker.\"\n",
    "        \"Follow the Task Guidelines.\"\n",
    "        \"Thinking out loud: <your thoughts about which interpretation is better>\"\n",
    "\n",
    "        \"### STRICT ANALYSIS PIPELINE\\n\"\n",
    "        \"Follow these steps to decode the mental state:\\n\\n\"\n",
    "\n",
    "        \"### Step 0: Fact\\n\"\n",
    "        \"- Given the fact: [She is smiling slightly.]\\n\"\n",
    "\n",
    "        \"1. **Vector Trajectory Tracing (The 'What')**:\\n\"\n",
    "        \"   - Compare the red arrow direction changes across timestamps.\\n\"\n",
    "        \"   - Is the arrow pointing at a fixed target ? OR is it pointing at different locations in each frame (e.g., Down -> Left -> Down-Right -> Straight)?\\n\"\n",
    "        \"   - **Key Question**: Define the 'Gaze Pattern'. Is it 'Stable/Focused' or 'Wandering/Unfocused'?\\n\\n\"\n",
    "        \n",
    "        \"2. **Incongruence Check (The 'Why')**:\\n\"\n",
    "        \"   - Observe her [facial expression] (Mouth/Cheeks). \\n\"\n",
    "        \"   - Contrast this with the 'Gaze Pattern' identified in Step 1. \\n\"\n",
    "        \"Is the expression consistent with the gaze behavior? or conflict?\\n\\n\"\n",
    "        \n",
    "        \"3. **Psychological Inference (The 'Meaning')**:\\n\"\n",
    "        \"   - Based on the specific [facial expression] + [Gaze Pattern] , what is her emotion?\\n\\n\"\n",
    "        \n",
    "        \"### Step 4: Final Inference\\n\"\n",
    "        \"- Based on the steps above, determine the mental state.\\n\"\n",
    "        \"- Choose between:\\n\"\n",
    "        \"   A. [Based on these observations, the speaker is likely not feeling joy.] \\n\"\n",
    "        \"   B. [Based on these observations, the speaker is likely feeling joy.] \\n\\n\"\n",
    "        \n",
    "        \"Output your reasoning for each step, and then provide the final conclusion.\"\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ¤– å‘é€ç»™ VLM æ¨ç†ä¸­...\")\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'images': frames_bytes\n",
    "            }],\n",
    "            options={\n",
    "                'temperature': 0.1,\n",
    "                'num_ctx': 30000\n",
    "            }\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(response['message']['content'])\n",
    "        print(\"=\"*30)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. æ™ºèƒ½æå–å…³é”®å¸§ (Gaze çªå˜ç‚¹) å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ\n",
    "    frames, idxs = process_video_smartly(VIDEO_PATH)\n",
    "    \n",
    "    # 2. VLM æ¨ç†\n",
    "    vlm_reasoning(frames, idxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VIBE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
